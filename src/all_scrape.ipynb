{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json as pyjson\n",
    "import time\n",
    "import uuid\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- √únvan ve ismi ayƒ±r ---\n",
    "def extract_title_and_name(raw_name):\n",
    "    known_titles = [\n",
    "        \"Prof. Dr.\", \"Do√ß. Dr.\", \"Dr. √ñƒür. √úyesi\", \"Yrd. Do√ß. Dr.\",\n",
    "        \"Uzm. Dr.\", \"Op. Dr.\", \"Dr. Dt.\", \"Dt.\", \"Dr.\"\n",
    "    ]\n",
    "    for t in known_titles:\n",
    "        if raw_name.startswith(t):\n",
    "            return t, raw_name[len(t):].strip()\n",
    "    return \"\", raw_name.strip()\n",
    "\n",
    "# --- Sayfa sayfa soru linklerini topla ---\n",
    "def collect_paginated_links(base_url, max_pages=100, max_links=2500):\n",
    "    links = set()\n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        url = base_url.format(page_num)\n",
    "        print(f\"üìÑ Sayfa {page_num} i≈üleniyor...\")\n",
    "        try:\n",
    "            res = requests.get(url, timeout=10)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            anchors = soup.select(\"a[href^='/blog/soru/']\")\n",
    "            for a in anchors:\n",
    "                href = a[\"href\"]\n",
    "                full = \"https://www.doktorsitesi.com\" + href\n",
    "                links.add(full)\n",
    "            print(f\"üîó Toplam link: {len(links)}\")\n",
    "            if len(links) >= max_links:\n",
    "                print(\"üõë Maksimum link sayƒ±sƒ±na ula≈üƒ±ldƒ±.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"[LINK ERROR] {url} -> {e}\")\n",
    "        time.sleep(1)\n",
    "    return list(links)\n",
    "\n",
    "# --- Soru sayfasƒ±nƒ± i≈üle ---\n",
    "def parse_qa_page(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "        script_tag = soup.find(\"script\", type=\"application/ld+json\")\n",
    "        if not script_tag:\n",
    "            raise ValueError(\"JSON-LD yok.\")\n",
    "        cleaned = re.sub(r\"[\\x00-\\x1F]+\", \" \", script_tag.string).strip()\n",
    "        data = pyjson.loads(cleaned)[0]\n",
    "\n",
    "        main = data[\"mainEntity\"]\n",
    "        accepted = main[\"acceptedAnswer\"]\n",
    "        topic = \"Fiziksel Tƒ±p ve Rehabilitasyon\"\n",
    "\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"title\": main.get(\"name\", \"\"),\n",
    "            \"question\": main.get(\"text\", \"\"),\n",
    "            \"answer\": accepted.get(\"text\", \"\"),\n",
    "            \"doctor_name\": accepted[\"author\"][\"name\"],\n",
    "            \"doctor_link\": accepted[\"author\"][\"url\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"[QA ERROR] {url} -> {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Doktor profilini i≈üle ---\n",
    "def parse_doctor_profile(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "        # Ad ve unvan ayƒ±r\n",
    "        raw_name = soup.find(\"h1\").text.strip()\n",
    "        title, name = extract_title_and_name(raw_name)\n",
    "\n",
    "        # Uzmanlƒ±k\n",
    "        spec_div = soup.select_one(\"div.expert-branches p\")\n",
    "        specialty = spec_div.text.strip() if spec_div else \"\"\n",
    "\n",
    "        # Klinik adƒ± (JSON-LD'den denenir)\n",
    "        script_tag = soup.find(\"script\", type=\"application/ld+json\")\n",
    "        clinic_name = rating = \"\"\n",
    "        if script_tag:\n",
    "            try:\n",
    "                cleaned = re.sub(r\"[\\x00-\\x1F]+\", \" \", script_tag.string).strip()\n",
    "                jsonld = pyjson.loads(cleaned)[0]\n",
    "                pos = jsonld.get(\"hasPOS\", [{}])[0]\n",
    "                full_clinic = pos.get(\"name\", \"\")\n",
    "                clinic_name = full_clinic.split(\",\", 1)[-1].strip() if \",\" in full_clinic else full_clinic.strip()\n",
    "                rating = jsonld.get(\"aggregateRating\", {}).get(\"ratingValue\", \"\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Adres: sokak\n",
    "        street = \"\"\n",
    "        street_el = soup.select_one(\"div.ta-address-explain\")\n",
    "        if street_el:\n",
    "            for c in street_el.contents:\n",
    "                if isinstance(c, str):\n",
    "                    street = c.strip()\n",
    "                    break\n",
    "\n",
    "        # ≈ûehir: URL'den\n",
    "        city = url.rstrip(\"/\").split(\"/\")[-1].capitalize()\n",
    "\n",
    "        # Posta Kodu: yoksa bo≈ü\n",
    "        post_code = \"\"\n",
    "\n",
    "        # Hakkƒ±nda\n",
    "        about = \"\"\n",
    "        about_section = soup.select_one(\"div#tabid-1 p\")\n",
    "        if about_section:\n",
    "            about = about_section.text.strip()\n",
    "\n",
    "        return {\n",
    "            \"Name\": name,\n",
    "            \"Title\": title,\n",
    "            \"Specialty\": specialty,\n",
    "            \"ClinicName\": clinic_name,\n",
    "            \"ClinicAddress\": {\n",
    "                \"Street\": street,\n",
    "                \"City\": city,\n",
    "                \"Post Code\": post_code\n",
    "            },\n",
    "            \"About\": about,\n",
    "            \"AverageReview\": rating\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"[DOCTOR ERROR] {url} -> {e}\")\n",
    "        return None\n",
    "\n",
    "# --- JSON kaydet ---\n",
    "def save_json(data, filename):\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    with open(os.path.join(\"output\", filename), \"w\", encoding=\"utf-8\") as f:\n",
    "        pyjson.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- Ana akƒ±≈ü ---\n",
    "def main():\n",
    "    print(\"üîç Soru linkleri toplanƒ±yor...\")\n",
    "    question_links = collect_paginated_links(\n",
    "      base_url=\"https://www.doktorsitesi.com/blog/sorular/fiziksel-tip-ve-rehabilitasyon?sayfa={}\",\n",
    "        max_pages=300,\n",
    "        max_links=2500\n",
    "    )\n",
    "\n",
    "    qa_data = []\n",
    "    doctor_data = {}\n",
    "\n",
    "    for idx, qurl in enumerate(question_links):\n",
    "        print(f\"[{idx+1}/{len(question_links)}] Soru i≈üleniyor...\")\n",
    "        qa = parse_qa_page(qurl)\n",
    "        if not qa:\n",
    "            continue\n",
    "\n",
    "        doc_url = qa[\"doctor_link\"]\n",
    "        if doc_url not in doctor_data:\n",
    "            print(f\"   üë®‚Äç‚öïÔ∏è Doktor i≈üleniyor: {doc_url}\")\n",
    "            doc_info = parse_doctor_profile(doc_url)\n",
    "            if not doc_info:\n",
    "                continue\n",
    "            doctor_id = str(uuid.uuid4())\n",
    "            doc_info[\"doctorID\"] = doctor_id\n",
    "            doctor_data[doc_url] = doc_info\n",
    "        else:\n",
    "            doctor_id = doctor_data[doc_url][\"doctorID\"]\n",
    "\n",
    "        qa_data.append({\n",
    "            \"topic\": qa[\"topic\"],\n",
    "            \"title\": qa[\"title\"],\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"answer\": qa[\"answer\"],\n",
    "            \"doctorID\": doctor_id\n",
    "        })\n",
    "\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"   ‚Ü™ {idx+1} soru i≈ülendi, 5 saniye mola...\")\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    save_json(qa_data, \"qa_data.json\")\n",
    "    save_json(list(doctor_data.values()), \"doctor_data.json\")\n",
    "    print(f\"\\n‚úÖ {len(qa_data)} soru ve {len(doctor_data)} doktor kaydedildi.\")\n",
    "\n",
    "# --- √áalƒ±≈ütƒ±r ---\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
